{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-VxPCkTDpjm"
      },
      "source": [
        "# LinkedIn Tech Jobs Data Visualization Project\n",
        "We find the dataset on Kaggle : https://www.kaggle.com/datasets/joebeachcapital/linkedin-jobs/data\n",
        "\n",
        "This project will use data visualization to explore the LinkedIn Tech Jobs dataset and identify key trends, such as the most in-demand skills, how demand has changed over time, and how demand varies across industries and job titles.\n",
        "\n",
        "Interactive data visualizations will allow users to explore the data in more detail.\n",
        "\n",
        "The goal is to provide insights into the tech job market that can be used to make informed decisions about careers.\n",
        "\n",
        "Examples of data visualizations you can find in our project:\n",
        "\n",
        "\n",
        "*   Correlation between the number of LinkedIn followers and the number of job\n",
        "*   The distribution of the number of applicants for the job listings\n",
        "*   The top 10 companies with the highest average number of applicants\n",
        "*   Distribution of data per city in India\n",
        "*   Pareto chart of data\n",
        "*   India City Distribution\n",
        "*   Ratio number candidat by employee (to know easy or difficult to access)\n",
        "*   Number of applicant by industry (to know the most popular industry)\n",
        "*   The 10 skills that appear the most\n",
        "*   Word Cloud of Job Titles in LinkedIn Tech Jobs Dataset\n",
        "\n",
        "In each code you can find short comments explaining briefly what we are doing, but you may find more detailed explanations in the text section just after the corresponding code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOlkwJhp5B2y",
        "outputId": "15ccd214-d876-4644-f6fb-d860d6607482"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\"apt-get\" no se reconoce como un comando interno o externo,\n",
            "programa o archivo por lotes ejecutable.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(Company_Name          0\n",
              " Class                 0\n",
              " Designation           0\n",
              " Location              0\n",
              " Total_applicants      0\n",
              " LinkedIn_Followers    0\n",
              " Level                 0\n",
              " Involvement           0\n",
              " Employee_count        0\n",
              " Industry              0\n",
              " PYTHON                0\n",
              " C++                   0\n",
              " JAVA                  0\n",
              " HADOOP                0\n",
              " SCALA                 0\n",
              " FLASK                 0\n",
              " PANDAS                0\n",
              " SPARK                 0\n",
              " NUMPY                 0\n",
              " PHP                   0\n",
              " SQL                   0\n",
              " MYSQL                 0\n",
              " CSS                   0\n",
              " MONGODB               0\n",
              " NLTK                  0\n",
              " TENSORFLOW            0\n",
              " LINUX                 0\n",
              " RUBY                  0\n",
              " JAVASCRIPT            0\n",
              " DJANGO                0\n",
              " REACT                 0\n",
              " REACTJS               0\n",
              " AI                    0\n",
              " UI                    0\n",
              " TABLEAU               0\n",
              " NODEJS                0\n",
              " EXCEL                 0\n",
              " POWER BI              0\n",
              " SELENIUM              0\n",
              " HTML                  0\n",
              " ML                    0\n",
              " dtype: int64,\n",
              "        Total_applicants  LinkedIn_Followers  Employee_count      PYTHON  \\\n",
              " count        809.000000        8.090000e+02      809.000000  809.000000   \n",
              " mean          23.400494        1.396695e+06     5165.627936    0.093943   \n",
              " std           35.130842        2.685066e+06     4192.477799    0.291930   \n",
              " min            0.000000        1.124500e+04      110.000000    0.000000   \n",
              " 25%            2.000000        8.086500e+04     1200.000000    0.000000   \n",
              " 50%            8.000000        2.702800e+05     5000.000000    0.000000   \n",
              " 75%           31.000000        7.930220e+05    10001.000000    0.000000   \n",
              " max          196.000000        1.199697e+07    10001.000000    1.000000   \n",
              " \n",
              "               C++        JAVA      HADOOP       SCALA       FLASK      PANDAS  \\\n",
              " count  809.000000  809.000000  809.000000  809.000000  809.000000  809.000000   \n",
              " mean     0.032138    0.206428    0.027194    0.173053    0.022250    0.003708   \n",
              " std      0.176477    0.404991    0.162749    0.378527    0.147586    0.060820   \n",
              " min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
              " 25%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
              " 50%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
              " 75%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
              " max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
              " \n",
              "        ...     REACTJS          AI          UI     TABLEAU      NODEJS  \\\n",
              " count  ...  809.000000  809.000000  809.000000  809.000000  809.000000   \n",
              " mean   ...    0.012361    0.867738    0.828183    0.018541    0.032138   \n",
              " std    ...    0.110559    0.338985    0.377455    0.134982    0.176477   \n",
              " min    ...    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
              " 25%    ...    0.000000    1.000000    1.000000    0.000000    0.000000   \n",
              " 50%    ...    0.000000    1.000000    1.000000    0.000000    0.000000   \n",
              " 75%    ...    0.000000    1.000000    1.000000    0.000000    0.000000   \n",
              " max    ...    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
              " \n",
              "             EXCEL    POWER BI    SELENIUM        HTML          ML  \n",
              " count  809.000000  809.000000  809.000000  809.000000  809.000000  \n",
              " mean     0.290482    0.018541    0.013597    0.111248    0.182942  \n",
              " std      0.454266    0.134982    0.115882    0.314634    0.386858  \n",
              " min      0.000000    0.000000    0.000000    0.000000    0.000000  \n",
              " 25%      0.000000    0.000000    0.000000    0.000000    0.000000  \n",
              " 50%      0.000000    0.000000    0.000000    0.000000    0.000000  \n",
              " 75%      1.000000    0.000000    0.000000    0.000000    0.000000  \n",
              " max      1.000000    1.000000    1.000000    1.000000    1.000000  \n",
              " \n",
              " [8 rows x 34 columns])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''Step 1 : importing libraries'''\n",
        "# Import Libraries to preprocesing Data\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import tkinter as tk\n",
        "from tkinter import ttk\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "%matplotlib inline\n",
        "import os\n",
        "import matplotlib as mpl\n",
        "if os.environ.get('DISPLAY','') == '':\n",
        "    print('no display found. Using non-interactive Agg backend')\n",
        "    mpl.use('Agg')\n",
        "\n",
        "### CREATE VIRTUAL DISPLAY ###\n",
        "!apt-get install -y xvfb # Install X Virtual Frame Buffer\n",
        "import os\n",
        "os.system('Xvfb :1 -screen 0 1600x1200x16  &')    # create virtual display with size 1600x1200 and 16 bit color. Color can be changed to 24 or 8\n",
        "os.environ['DISPLAY']=':1.0'    # tell X clients to use our virtual DISPLAY :1.0.\n",
        "\n",
        "\"\"\"\n",
        "# If someone wants to try it with Numpy\n",
        "url = 'https://raw.githubusercontent.com/Jhonnatan7br/LinkedIn-Tech/main/LinkedIn%20Tech%20jobs%20-%20Informatic%20and%20Telecoms.csv'\n",
        "df = np.genfromtxt(url, delimiter=',', skip_header=1, dtype=None, encoding=None)\n",
        "print(df)\n",
        "\"\"\"\n",
        "url = 'https://raw.githubusercontent.com/Jhonnatan7br/LinkedIn-Tech/main/LinkedIn%20Tech%20jobs%20-%20Informatic%20and%20Telecoms.csv'\n",
        "df = pd.read_csv(url,  sep=\",\")\n",
        "\n",
        "\"\"\"Step 2 :Explore the DataFrame to get an understanding of the data:\"\"\"\n",
        "# Display all the DataSet (It cant show all because it is too large)\n",
        "df.head()\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "#df.head()\n",
        "#df.describe()\n",
        "\n",
        "\"\"\" Step 3 : Clean the data:\"\"\"\n",
        "# Check for missing values in the dataset\n",
        "missing_values = df.isnull().sum()\n",
        "# Remove any duplicate rows\n",
        "df = df.drop_duplicates()\n",
        "# Remove any rows with missing values\n",
        "df = df.dropna()\n",
        "\n",
        "# Get a summary of the numerical columns\n",
        "numerical_summary = df.describe()\n",
        "missing_values, numerical_summary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xU9QNo9FOTjy"
      },
      "source": [
        "# Additional details and explanations :\n",
        " **Step 1 : Importing libraries**\n",
        "\n",
        "> The import statements introduce external libraries that provide specific functionalities:\n",
        "\n",
        "\n",
        "*   requests: Makes HTTP requests for interacting with web APIs and retrieving data.\n",
        "*   numpy: Performs efficient numerical operations and data manipulation.\n",
        "*   pandas: Analyzes and manipulates tabular data, time series, and statistics.\n",
        "*   matplotlib.pyplot: Creates 2D visualizations like line plots, scatter plots, and bar charts.\n",
        "*   seaborn: Builds on matplotlib to create visually appealing and informative data visualizations.\n",
        "\n",
        "**Step 2 : Explore the DataFrame**\n",
        "\n",
        "> The provided code snippet represents the second step in data analysis, which involves exploring the DataFrame to gain an understanding of the data's structure, content, and characteristics.\n",
        "\n",
        "* print(df.head()): This line displays the first few rows of the DataFrame, providing a glimpse into the data's format and the types of information it contains.\n",
        "\n",
        "* df.describe(): While not explicitly executed in the code, the df.describe() method provides summary statistics for each numerical column in the DataFrame. This includes measures like mean, median, standard deviation, and quartiles, helping to understand the distribution and central tendency of the data.\n",
        "\n",
        ">By exploring the DataFrame using these methods, data analysts can identify patterns, anomalies, and potential relationships between variables, laying the foundation for further analysis and exploration.\n",
        "\n",
        "**Step 3 : Clean the data**\n",
        "> The provided code snippet represents the third step in data analysis, which involves cleaning and preparing the data for further analysis.\n",
        "\n",
        "* Check for missing values: The df.isnull().sum() method calculates the number of missing values (NA or NaN) in each column of the DataFrame. The resulting Series, missing_values, holds the column names as its index and the corresponding number of missing values as values. This information can help identify columns with a significant amount of missing data that may require further attention.\n",
        "\n",
        "* Remove duplicate rows: The df.drop_duplicates() method eliminates duplicate rows in the DataFrame. Duplicate rows are identified based on all columns in the DataFrame, ensuring that each unique combination of values is represented only once. This step can help remove redundant data and ensure that subsequent analysis is based on a distinct set of records.\n",
        "\n",
        "* Remove rows with missing values: The df.dropna() method removes rows with at least one missing value. This step eliminates incomplete records that may introduce errors or biases in the analysis. However, it is important to consider the impact of removing a significant portion of the data and whether imputation techniques could be used to address missing values instead.\n",
        "\n",
        ">Get a summary of the numerical columns: The df.describe() method provides summary statistics for each numerical column in the DataFrame. This includes measures like mean, median, standard deviation, and quartiles, helping to understand the distribution and central tendency of the data. This information can be used to identify potential outliers, detect skewed distributions, and assess the overall variability of the numerical variables.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Uq-fgmYxV3_B"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-9-7aae6bdcb77d>:35: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "  fig, ax = plt.subplots(figsize=(10, 6))\n"
          ]
        }
      ],
      "source": [
        "# Create a function to generate the plot\n",
        "# Modern Color Scheme\n",
        "bg_color = \"#f5f5f5\"  # Light grey background\n",
        "fg_color = \"#333333\"  # Dark text\n",
        "accent_color = \"#007bff\"  # Blue accent\n",
        "\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import tkinter as tk\n",
        "from tkinter import ttk\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\n",
        "import matplotlib\n",
        "\n",
        "# Sample DataFrame\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'Location': ['City1', 'City2', 'City3', 'City4', 'City5', 'City1', 'City2', 'City3']\n",
        "})\n",
        "\n",
        "# Modern Color Scheme\n",
        "bg_color = \"#f5f5f5\"  # Light grey background\n",
        "fg_color = \"#333333\"  # Dark text\n",
        "accent_color = \"#007bff\"  # Blue accent\n",
        "\n",
        "def plot_distribution():\n",
        "    # Get the top 5 cities in India with the highest frequency from the 'Location' column\n",
        "    Top_5_cities_india = df['Location'].value_counts().sort_values(ascending=False).head(5)\n",
        "\n",
        "    # Create a figure with a specific size\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "    # Create a bar plot with the top 5 cities on the x-axis and their frequencies on the y-axis\n",
        "    sns.barplot(x=Top_5_cities_india.index, y=Top_5_cities_india.values, ax=ax)\n",
        "\n",
        "    # Add labels to the bars\n",
        "    for p in ax.patches:\n",
        "        ax.annotate(f'{p.get_height():.0f}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                    ha='center', va='center', fontsize=12, color='black', xytext=(0, 5),\n",
        "                    textcoords='offset points')\n",
        "\n",
        "    # Rotate x-axis labels for better readability\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    # Set labels for the x and y axes\n",
        "    plt.xlabel('Cities')\n",
        "    plt.ylabel('Frequency')\n",
        "\n",
        "    # Set the title of the plot\n",
        "    plt.title('Distribution of Data per City in India - Top 5')\n",
        "\n",
        "    # Customize the plot style (optional)\n",
        "    sns.set(style=\"whitegrid\")\n",
        "\n",
        "    # Display the plot in a canvas widget\n",
        "    canvas = FigureCanvasTkAgg(fig, master=window)\n",
        "    canvas_widget = canvas.get_tk_widget()\n",
        "    canvas_widget.pack()\n",
        "\n",
        "\n",
        "    # Create a Tkinter window\n",
        "window = tk.Tk()\n",
        "window.title(\"Data Distribution in Indian Cities\")\n",
        "window.configure(bg=bg_color)\n",
        "\n",
        "# Styling Buttons\n",
        "style = ttk.Style()\n",
        "style.configure('TButton', font=('Arial', 10), borderwidth='1')\n",
        "style.map('TButton', foreground=[('active', accent_color)], background=[('active', bg_color)])\n",
        "\n",
        "# Create a button to trigger the plot function\n",
        "plot_button = ttk.Button(window, text=\"Plot Data\", command=plot_distribution)\n",
        "plot_button.pack()\n",
        "\n",
        "# Adding Buttons\n",
        "refresh_button = ttk.Button(window, text=\"Refresh Data\", command=plot_distribution)\n",
        "refresh_button.pack(pady=10)\n",
        "\n",
        "export_button = ttk.Button(window, text=\"Export Chart\")\n",
        "export_button.pack(pady=10)\n",
        "\n",
        "# Calling the function to display the plot initially\n",
        "plot_distribution()\n",
        "\n",
        "# Start the Tkinter main loop\n",
        "window.mainloop()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 586
        },
        "id": "HTEHN360GtST",
        "outputId": "a08d6427-5b51-4a91-ac64-a27a65a8374d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-3-866105df5b23>:25: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
            "  plt.show()\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "-0.31828242750821"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''Correlation between the number of LinkedIn followers and the number of job'''\n",
        "\n",
        "'''Step 1 : '''\n",
        "# Get the number of job listings of each company\n",
        "num_job_listings = df['Company_Name'].value_counts()\n",
        "\n",
        "'''step 2 :'''\n",
        "# Get the number of LinkedIn followers of each company\n",
        "num_followers = df.drop_duplicates(subset='Company_Name')[['Company_Name', 'LinkedIn_Followers']].set_index('Company_Name')\n",
        "\n",
        "'''step 3 :'''\n",
        "# Merge the two DataFrames\n",
        "followers_vs_jobs = pd.merge(num_followers, num_job_listings, left_index=True, right_index=True)\n",
        "followers_vs_jobs.columns = ['LinkedIn_Followers', 'Num_Job_Listings']\n",
        "\n",
        "'''step 4 :'''\n",
        "# Plot the correlation between the number of LinkedIn followers and the number of job listings of a company\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x='LinkedIn_Followers', y='Num_Job_Listings', data=followers_vs_jobs)\n",
        "plt.title('Correlation Between LinkedIn Followers and Number of Job Listings')\n",
        "plt.xlabel('Number of LinkedIn Followers')\n",
        "plt.ylabel('Number of Job Listings')\n",
        "plt.xscale('log')\n",
        "plt.yscale('log')\n",
        "plt.show()\n",
        "\n",
        "'''step 5 :'''\n",
        "# Calculate the correlation coefficient\n",
        "correlation = followers_vs_jobs.corr().iloc[0, 1]\n",
        "correlation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5TEbwwbp-M7"
      },
      "source": [
        "# Additional details and explanations :\n",
        "\n",
        "**Step 1 : Get the number of job listings of each company**\n",
        "\n",
        "\n",
        ">This line of code aims to determine the frequency of each company's appearance within the Company_Name column of the DataFrame df. The value_counts() function effectively counts the number of occurrences for each unique value in the specified column. The resulting Series, stored in num_job_listings, holds the company names as its index and their corresponding job listing counts as values.\n",
        "\n",
        "**Step 2 : Get the number of LinkedIn followers of each company**\n",
        "\n",
        "> This step focuses on extracting unique companies and their corresponding LinkedIn follower counts from the DataFrame df. The drop_duplicates() function eliminates duplicate entries based on the Company_Name column, ensuring that only distinct companies remain. Next, the relevant columns, Company_Name and LinkedIn_Followers, are selected using double square brackets. Finally, the set_index() method sets the Company_Name column as the index of the resulting DataFrame, num_followers.\n",
        "\n",
        "**Step 3 : Merge the two DataFrames**\n",
        "\n",
        "> This step involves merging the two DataFrames, num_followers and num_job_listings, based on their common index, Company_Name. The pd.merge() function performs the merging operation, and the left_index=True and right_index=True parameters specify that the index values from both DataFrames should be used as the basis for the merge. The resulting DataFrame, followers_vs_jobs, contains the combined information from both DataFrames.\n",
        "\n",
        "\n",
        "**Step 4 : Plot the correlation between the number of LinkedIn followers and the number of job listings of a company**\n",
        "\n",
        "> This step creates a scatter plot to visualize the relationship between the number of LinkedIn followers and the number of job listings for each company. The plt.figure() function sets the figure size, while sns.scatterplot() generates the scatter plot with the specified data from followers_vs_jobs.\n",
        "\n",
        "> The plot's title, axis labels, and x and y-axis scaling are customized using plt.title(), plt.xlabel(), plt.ylabel(), plt.xscale(), and plt.yscale(), respectively. The log scale is applied to both axes to better represent the wide range of values in the data. Finally, the plot is displayed using plt.show().\n",
        "\n",
        "> sccter plot : Usually we use a scatter plot for to visualize the relationship between two variables, to identify trends and patterns, to detect outliers, to compare groups of data or to communicate insights clearly.\n",
        ">> We choose to use a scatter plot here because it would allow us to visually assess the strength and direction of the correlation. If there is a positive correlation, we would expect to see a general upward trend in the data points, indicating that companies with more LinkedIn followers tend to have more job listings. Conversely, a negative correlation would be indicated by a downward trend, suggesting that companies with more LinkedIn followers tend to have fewer job listings.\n",
        "\n",
        "**Step 5 : Calculate the correlation coefficient**\n",
        "\n",
        "> This final step calculates the correlation coefficient between the number of LinkedIn followers and the number of job listings. The corr() method applied to followers_vs_jobs returns a Correlation matrix. The correlation coefficient for the desired variables, LinkedIn_Followers and Num_Job_Listings, is extracted using indexing and stored in the variable correlation.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "oQUtvToTHkcX",
        "outputId": "c9bafdea-88c1-4ab4-f57b-30ffba521da2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-4-0dddf5700ec3>:9: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
            "  plt.show()\n"
          ]
        }
      ],
      "source": [
        "'''The distribution of the number of applicants for the job listings'''\n",
        "\n",
        "# Plot the distribution of the number of applicants for the job listings\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df['Total_applicants'], kde=True)\n",
        "plt.title('Distribution of Number of Applicants')\n",
        "plt.xlabel('Number of Applicants')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4fgenF9Zq0W"
      },
      "source": [
        "# Additional details and explanations :\n",
        "\n",
        ">This code snippet creates a histogram and kernel density estimate (KDE) plot of the Total_applicants column in the DataFrame df. The histplot() function is used to create the plot, and the kde parameter is set to True to enable the KDE plot.\n",
        "\n",
        ">A KDE plot is a smoothed version of a histogram. It is created by estimating the probability density function (PDF) of the data and then plotting the PDF. The PDF is a function that describes the probability of a data point occurring at a given value. The KDE plot superimposed on the histogram helps to smooth out the distribution and provides a more accurate representation of the underlying distribution of the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "uSt5emC4IqRj",
        "outputId": "2a9867e0-219e-4dab-a84e-707d6e9aeb2f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-5-5544a6f065b6>:18: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
            "  plt.show()\n"
          ]
        }
      ],
      "source": [
        "'''The top 10 companies with the highest average number of applicants'''\n",
        "\n",
        "'''Step 1 :'''\n",
        "# Get the average number of applicants for each company\n",
        "avg_applicants_per_company = df.groupby('Company_Name')['Total_applicants'].mean()\n",
        "\n",
        "'''Step 2 :'''\n",
        "# Get the top 10 companies with the highest average number of applicants\n",
        "top_10_companies_applicants = avg_applicants_per_company.sort_values(ascending=False).head(10)\n",
        "\n",
        "'''Step 3 :'''\n",
        "# Plot the top 10 companies with the highest average number of applicants\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=top_10_companies_applicants.values, y=top_10_companies_applicants.index)\n",
        "plt.title('Top 10 Companies with Highest Average Number of Applicants')\n",
        "plt.xlabel('Average Number of Applicants')\n",
        "plt.ylabel('Company')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRptIWwPgaX6"
      },
      "source": [
        "# Additional details and explanations :\n",
        "\n",
        "**Step 1: Get the average number of applicants for each company**\n",
        "\n",
        ">This step calculates the average number of applicants for each company. The groupby() function is used to group the DataFrame by company name, and the mean() function is used to calculate the average number of applicants for each company. The result is a Series called avg_applicants_per_company.\n",
        "\n",
        "**Step 2: Get the top 10 companies with the highest average number of applicants**\n",
        "\n",
        ">This step sorts the avg_applicants_per_company Series in descending order and selects the top 10 companies. The sort_values() function is used to sort the Series, and the head() function is used to select the top 10 companies. The result is a Series called top_10_companies_applicants.\n",
        "\n",
        "**Step 3: Plot the top 10 companies with the highest average number of applicants**\n",
        "\n",
        ">Same as previous data visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        },
        "id": "vEyxsZsg3xN9",
        "outputId": "db89869d-29f5-4819-f950-d329838857e0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-6-ac5b8933cc10>:25: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
            "  plt.show()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Maharashtra      204\n",
            " Telangana        166\n",
            " Karnataka        148\n",
            " Tamil Nadu        59\n",
            " Uttar Pradesh     54\n",
            "Name: Location, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "'''Distribution of the data according to cities in India'''\n",
        "\n",
        "'''Step 1'''\n",
        "# Get the top 5 cities in India with the highest frequency from the 'Location' column\n",
        "Top_5_cities_india = df['Location'].value_counts().sort_values(ascending=False).head(5)\n",
        "\n",
        "# Print the top 5 cities and their frequencies\n",
        "print(Top_5_cities_india)\n",
        "\n",
        "'''Step 2'''\n",
        "# Create a figure with a specific size\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Create a bar plot with the top 5 cities on the x-axis and their frequencies on the y-axis\n",
        "sns.barplot(x=Top_5_cities_india.index, y=Top_5_cities_india.values)\n",
        "\n",
        "# Set labels for the x and y axes\n",
        "plt.xlabel('Locations')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "# Set the title of the plot\n",
        "plt.title('Distribution of Data per City in India - Top 5')\n",
        "\n",
        "# Display the bar plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yw-TkxCVaULG"
      },
      "source": [
        "# Additional details and explanations :\n",
        "\n",
        "**Step 1 : Get the top 5 cities in India with the highest frequency from the 'Location' column**\n",
        "> This code analyzes the geographical distribution of job applications in the LinkedIn Tech Jobs Dataset, specifically focusing on India. It identifies the top five cities in India with the highest number of job applications. The code first counts the number of job applications from each city in the dataset using the value_counts() function. Then, it sorts the resulting Series in descending order to identify the top five cities. Finally, it displays the results using the print() function.\n",
        "\n",
        "**Step 2 : Data visualization**\n",
        "> Same as previous data visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "id": "KhjV4dFqIK1Z",
        "outputId": "adb64856-bb42-4d7d-afd2-f08eff213197"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Maharashtra        25.216316\n",
            " Telangana          45.735476\n",
            " Karnataka          64.029666\n",
            " Tamil Nadu         71.322621\n",
            " Uttar Pradesh      77.997528\n",
            " Delhi              83.807169\n",
            " Haryana            87.639061\n",
            " West Bengal        90.976514\n",
            " Rajasthan          93.077874\n",
            " Gujarat            94.808405\n",
            " Kerala             96.044499\n",
            " Odisha             97.156984\n",
            " India              98.269468\n",
            " Madhya Pradesh     99.134734\n",
            " Andhra Pradesh     99.876391\n",
            " Punjab            100.000000\n",
            "Name: Location, dtype: float64\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-7-e3472255a95c>:30: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
            "  plt.show()\n"
          ]
        }
      ],
      "source": [
        "'''Cumulative data graph in a pareto'''\n",
        "\n",
        "'''Step 1'''\n",
        "# Calculate the cumulative percentage data for city distribution\n",
        "cumulative_data = np.cumsum(df['Location'].value_counts().sort_values(ascending=False)) / np.sum(df['Location'].value_counts().sort_values(ascending=False)) * 100\n",
        "\n",
        "# Print the cumulative data\n",
        "print(cumulative_data)\n",
        "\n",
        "'''Step 2'''\n",
        "# Create a figure with a specific size\n",
        "plt.figure(figsize=(30, 6))\n",
        "\n",
        "'''Step 3'''\n",
        "# Get the city names in descending order of frequency\n",
        "city_names = df['Location'].value_counts().sort_values(ascending=False).index\n",
        "\n",
        "'''Step 4'''\n",
        "# Create a bar plot with city names on the x-axis and cumulative data on the y-axis\n",
        "plt.bar(city_names, cumulative_data)\n",
        "\n",
        "# Set labels for the x and y axes\n",
        "plt.xlabel('City')\n",
        "plt.ylabel('Cumulative Data (%)')\n",
        "\n",
        "# Set the title of the Pareto Chart\n",
        "plt.title('Pareto Chart of Data')\n",
        "\n",
        "# Display the Pareto Chart\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iT8aDihIiXKN"
      },
      "source": [
        "# Additional details and explanations :\n",
        "**Step 1 : Calculate the cumulative percentage data for city distribution**\n",
        "> Sort City Counts: The frequency of each city is first calculated using the value_counts() function. Then, the city counts are sorted in descending order to identify the cities with the highest number of job applications.\n",
        "\n",
        ">Calculate Cumulative Sum: The np.cumsum() function is used to calculate the cumulative sum of the sorted city counts. This represents the total number of job applications accumulated up to that point.\n",
        "\n",
        ">Normalize Cumulative Sum: The cumulative sum is then divided by the total number of job applications and multiplied by 100 to express the data as percentages. This results in a series of cumulative percentages representing the proportion of job applications accounted for by each city up to that point.\n",
        "\n",
        "**Step 2 : Create a figure with a specific size**\n",
        ">This step simply prints the calculated cumulative data series to the console. This provides a numerical representation of the distribution of job applications across cities.\n",
        "\n",
        "**Step 3 : Get the city names in descending order of frequency**\n",
        ">Extract City Names: The city names are extracted from the Location column of the DataFrame using the .index property.\n",
        "\n",
        ">Sort City Names: The city names are sorted in descending order of frequency using the sort_values() function. This ensures that the cities with the highest number of job applications are processed first in the subsequent steps.\n",
        "\n",
        "**Step 4: Data visualization**\n",
        "> Same as previous data visualizations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "id": "c5oDnyGWLsyJ",
        "outputId": "f4ec882c-1c5b-4242-a46c-5fccd1e11859"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-8-d968ca2be37c>:47: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
            "  plt.show()\n"
          ]
        }
      ],
      "source": [
        "'''India City Distribution'''\n",
        "\n",
        "'''Step 1'''\n",
        "# Create a subplot with a specific aspect ratio\n",
        "fig, ax = plt.subplots(figsize=(6, 3), subplot_kw=dict(aspect=\"equal\"))\n",
        "\n",
        "'''Step 2'''\n",
        "# Get the top 4 locations with the highest frequency from the 'Location' column\n",
        "labels = df['Location'].value_counts().sort_values(ascending=False).head(4).index\n",
        "\n",
        "# Get the corresponding counts for the top 4 locations\n",
        "values = df['Location'].value_counts().sort_values(ascending=False).tolist()[:4]\n",
        "\n",
        "'''Step 3'''\n",
        "# Create a pie chart with specified parameters\n",
        "wedges, texts, autotexts = ax.pie(values, wedgeprops=dict(width=0.65), startangle=-40, autopct='%.0f%%')\n",
        "\n",
        "# Define properties for the annotation box\n",
        "bbox_props = dict(boxstyle=\"square,pad=0.3\", fc=\"w\", ec=\"k\", lw=0.72)\n",
        "\n",
        "# Create a dictionary of keyword arguments (kw) for annotation\n",
        "kw = dict(arrowprops=dict(arrowstyle=\"-\"),\n",
        "          bbox=bbox_props, zorder=0, va=\"center\")\n",
        "\n",
        "# Loop through the wedges (slices) in the pie chart\n",
        "for i, p in enumerate(wedges):\n",
        "    # Calculate the angle at the center of the wedge\n",
        "    ang = (p.theta2 - p.theta1) / 2. + p.theta1\n",
        "    y = np.sin(np.deg2rad(ang))\n",
        "    x = np.cos(np.deg2rad(ang))\n",
        "\n",
        "    # Determine the horizontal alignment of the annotation\n",
        "    horizontalalignment = {-1: \"right\", 1: \"left\"}[int(np.sign(x))]\n",
        "\n",
        "    # Define the connection style for the annotation arrow\n",
        "    connectionstyle = f\"angle,angleA=0,angleB={ang}\"\n",
        "\n",
        "    # Update the arrowprops and annotate the label\n",
        "    kw[\"arrowprops\"].update({\"connectionstyle\": connectionstyle})\n",
        "    ax.annotate(labels[i], xy=(x, y), xytext=(1.35*np.sign(x), 1.4*y),\n",
        "                horizontalalignment=horizontalalignment, **kw)\n",
        "\n",
        "# Set the title of the pie chart\n",
        "ax.set_title(\"India City Distribution\")\n",
        "\n",
        "# Display the pie chart\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGjnEoQ70y2t"
      },
      "source": [
        "# More details and explanations :\n",
        "\n",
        "**Step 1: Figure and Axes Setup:**\n",
        "\n",
        ">fig, ax = plt.subplots(figsize=(6, 3), subplot_kw=dict(aspect=\"equal\")): This line creates a figure with a single subplot and sets its size to 6 inches in width and 3 inches in height. It also ensures that the aspect ratio of the plot is equal, preserving the circular shape of the donut chart.\n",
        "\n",
        "**Step 2: Data Preparation:**\n",
        "\n",
        "* labels = df['Location'].value_counts().sort_values(ascending=False).head(4).index: This line identifies the top four cities by sorting the Location column's value counts in descending order and selecting the top four labels. These labels represent the city names.\n",
        "\n",
        "* values = df['Location'].value_counts().sort_values(ascending=False).tolist()[:4]: This line extracts the corresponding values for the top four cities based on the labels. These values represent the frequencies of each city.\n",
        "\n",
        "**Step 3 : Creating the Donut Chart:**\n",
        "\n",
        "> wedges, texts, autotexts= ax.pie(values, wedgeprops=dict(width=0.65), startangle=-40, autopct='%.0f%%'): This line generates the donut chart using the ax.pie() function. It takes the values list as input and sets the wedgeprops dictionary to define the width and appearance of the pie slices. The startangle parameter rotates the chart to start at the specified angle. The autopct parameter displays the percentage contribution of each slice inside the chart.\n",
        "Label Placement Customization:\n",
        "\n",
        "> bbox_props = dict(boxstyle=\"square,pad=0.3\", fc=\"w\", ec=\"k\", lw=0.72): This line defines the properties of the label boxes, including their style, padding, fill color, edge color, and line width.\n",
        "\n",
        "> kw = dict(arrowprops=dict(arrowstyle=\"-\"), bbox=bbox_props, zorder=0, va=\"center\"): This line defines the properties of the arrows connecting the labels to the respective pie slices. It sets the arrow style to a simple line, uses the previously defined bbox_props for the label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        },
        "id": "vABFwRPfbka0",
        "outputId": "877827ca-456d-424b-abd0-d48b01ac96e3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-9-1edb9f17b7b2>:29: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
            "  plt.show()\n"
          ]
        }
      ],
      "source": [
        "'''Ratio number candidat by employee (to know easy or difficult to access)'''\n",
        "\n",
        "'''Step 1 :'''\n",
        "# Group the data by company, location and profession, then calculate the total number of applicants\n",
        "grouped_df = df.groupby(['Company_Name', 'Location', 'Designation']).agg({\n",
        "    'Total_applicants': 'sum',\n",
        "    'Employee_count': 'mean'\n",
        "}).reset_index()\n",
        "\n",
        "'''Step 2 :'''\n",
        "# Calculate the ratio of candidates per employee\n",
        "grouped_df['Applicants per employee ratio'] = grouped_df['Total_applicants'] / grouped_df['Employee_count']\n",
        "\n",
        "'''Step 3 :'''\n",
        "# Sort data by ratio\n",
        "grouped_df = grouped_df.sort_values(by='Applicants per employee ratio', ascending=False)\n",
        "\n",
        "'''Step 4 :'''\n",
        "# Create graphic\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(grouped_df['Company_Name'], grouped_df['Applicants per employee ratio'], color='skyblue')\n",
        "plt.xlabel('Company')\n",
        "plt.ylabel('Applicants per employee ratio')\n",
        "plt.title('Applicants per employee ratio')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "\n",
        "# Plot graph\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSrEwYDO1YKg"
      },
      "source": [
        "# More details and explanations :\n",
        "\n",
        "**Step 1 : Group the data by company, location and profession, then calculate the total number of applicants**\n",
        "> This code segment groups the data in the DataFrame df by three columns: Company_Name, Location, and Designation. It then applies aggregation functions to calculate the desired statistics for each group.\n",
        "\n",
        "* 'Total_applicants': 'sum': This calculates the total number of applicants for each combination of company, location, and designation.\n",
        "\n",
        "* 'Employee_count': 'mean': This calculates the average number of employees for each combination of company, location, and designation.\n",
        "\n",
        "* The reset_index() method is applied to the resulting grouped DataFrame to convert the group indices back into regular columns.\n",
        "\n",
        "**Step 2 : Calculate the ratio of candidates per employee**\n",
        "\n",
        ">This line calculates the ratio of applicants per employee for each group by dividing the total number of applicants by the average number of employees. This ratio reflects the competition for each position and can be used to identify companies or locations with high or low demand.\n",
        "\n",
        "**Step 3 : Sort data by ratio**\n",
        "> This line sorts the grouped DataFrame by the calculated Applicants per employee ratio in descending order. This arrangement allows for easy identification of companies with the highest ratios.\n",
        "\n",
        "**Step 4 : Create graphic**\n",
        "> same as previous data visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "id": "coFjmgSfbwpy",
        "outputId": "57d2c0d3-5eae-406b-bf0e-8b586e1c3a48"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-10-d30d4ce1aed5>:14: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
            "  plt.show()\n"
          ]
        }
      ],
      "source": [
        "'''Number of applicant by industry (to know the most popular industry)'''\n",
        "\n",
        "# Calculate total number of applicants per industry\n",
        "df_industries = df.groupby(\"Industry\")[\"Total_applicants\"].sum()\n",
        "\n",
        "# Display bar chart\n",
        "df_industries.plot(kind=\"bar\")\n",
        "\n",
        "# Add labels to axes\n",
        "plt.xlabel(\"Industry\")\n",
        "plt.ylabel(\"Number of applicants\")\n",
        "\n",
        "# Display graph\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqyeMaTCwGNg",
        "outputId": "f079e587-3c1b-4725-d983-3666d216361b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Company_Name', 'Class', 'Designation', 'Location', 'Total_applicants', 'LinkedIn_Followers', 'Level', 'Involvement', 'Employee_count', 'Industry', 'PYTHON', 'C++', 'JAVA', 'HADOOP', 'SCALA', 'FLASK', 'PANDAS', 'SPARK', 'NUMPY', 'PHP', 'SQL', 'MYSQL', 'CSS', 'MONGODB', 'NLTK', 'TENSORFLOW', 'LINUX', 'RUBY', 'JAVASCRIPT', 'DJANGO', 'REACT', 'REACTJS', 'AI', 'UI', 'TABLEAU', 'NODEJS', 'EXCEL', 'POWER BI', 'SELENIUM', 'HTML', 'ML']\n"
          ]
        }
      ],
      "source": [
        "'''top 10 of skills which appear the most in the LinkedIn Tech Job Dataset'''\n",
        "\n",
        "'''step 1'''\n",
        "\n",
        "# First I print the names of of all the columns so I can copy and paste the names of the columns that I am interested in\n",
        "\n",
        "column_names = df.columns.tolist()\n",
        "print(column_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 838
        },
        "id": "wpHuhkQJS4n6",
        "outputId": "61e5cd3f-4776-4eb6-ed7b-35946912ec5d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-12-f15137daa59b>:26: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
            "  plt.show()\n"
          ]
        }
      ],
      "source": [
        "'''step 2'''\n",
        "\n",
        "skills = ['PYTHON', 'C++', 'JAVA', 'HADOOP', 'SCALA', 'FLASK', 'PANDAS', 'SPARK', 'NUMPY', 'PHP', 'SQL', 'MYSQL', 'CSS', 'MONGODB', 'NLTK', 'TENSORFLOW', 'LINUX', 'RUBY', 'JAVASCRIPT', 'DJANGO', 'REACT', 'REACTJS', 'AI', 'UI', 'TABLEAU', 'NODEJS', 'EXCEL', 'POWER BI', 'SELENIUM', 'HTML', 'ML']\n",
        "\n",
        "colum_sum = df[skills].sum()\n",
        "#calculates the percentage of each skill\n",
        "skill_percentages = (colum_sum / colum_sum.sum()) * 100\n",
        "\n",
        "#print(skill_percentages)\n",
        "\n",
        "'''step 3'''\n",
        "\n",
        "# Create a pie chart of the top 10 skill percentages\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.pie(\n",
        "    skill_percentages.values[:10],  # Considering only the top 10 skills\n",
        "    labels=skill_percentages.index[:10],\n",
        "    autopct=\"%1.1f%%\",  # Display percentage values\n",
        ")\n",
        "\n",
        "plt.title(\"Percentage of Top 10 Skills in the LinkedIn Tech Jobs Dataset\")\n",
        "\n",
        "# Create a legend for the pie chart\n",
        "plt.legend(skill_percentages.index[:10], loc=\"upper right\", bbox_to_anchor=(1.05, 1.05), title=\"Skills\")\n",
        "\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9OAqzul-fWE"
      },
      "source": [
        "# Additional details and explanations :\n",
        "\n",
        "**Step 1**\n",
        "> prints the names of all of the columns in the DataFrame. This is useful because it allows us to see which columns contain the data that we are interested in.\n",
        "\n",
        "**Step 2 : Calculates the percentage of each skill**\n",
        "\n",
        "> This step calculates the percentage of each skill in the LinkedIn Tech Jobs Dataset. The percentage of each skill is calculated by dividing the number of applicants for that skill by the total number of applicants. The sum() function is used to sum the number of applicants for each skill, and the div() function is used to divide the sum of the number of applicants for each skill by the total number of applicants.\n",
        "\n",
        "**Step 3 : Create a pie chart of the top 10 skill percentages**\n",
        "> This step creates a pie chart of the percentage of the top 10 skills in the LinkedIn Tech Jobs Dataset. The pie() function is used to create the pie chart, and the figsize() function is used to set the size of the figure. The autopct parameter is used to display the percentage values on the pie chart. The title() function is used to set the title of the pie chart, and the legend() function is used to create a legend for the pie chart."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSOzJ2OJ597g",
        "outputId": "ec5a5ddd-ebab-4c20-9b32-3482e6aebfbd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting wordcloud\n",
            "  Downloading wordcloud-1.9.2-cp39-cp39-win_amd64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\cristianmedina\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from wordcloud) (1.21.1)\n",
            "Requirement already satisfied: pillow in c:\\users\\cristianmedina\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from wordcloud) (10.0.0)\n",
            "Requirement already satisfied: matplotlib in c:\\users\\cristianmedina\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from wordcloud) (3.5.0)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\cristianmedina\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib->wordcloud) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\cristianmedina\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib->wordcloud) (4.28.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\cristianmedina\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib->wordcloud) (1.3.2)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\cristianmedina\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib->wordcloud) (23.2)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\cristianmedina\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib->wordcloud) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\cristianmedina\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib->wordcloud) (2.8.1)\n",
            "Requirement already satisfied: setuptools-scm>=4 in c:\\users\\cristianmedina\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib->wordcloud) (6.3.2)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\cristianmedina\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.15.0)\n",
            "Requirement already satisfied: setuptools in c:\\users\\cristianmedina\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from setuptools-scm>=4->matplotlib->wordcloud) (56.0.0)\n",
            "Requirement already satisfied: tomli>=1.0.0 in c:\\users\\cristianmedina\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from setuptools-scm>=4->matplotlib->wordcloud) (1.2.2)\n",
            "Downloading wordcloud-1.9.2-cp39-cp39-win_amd64.whl (153 kB)\n",
            "   ---------------------------------------- 153.3/153.3 kB 2.3 MB/s eta 0:00:00\n",
            "Installing collected packages: wordcloud\n",
            "Successfully installed wordcloud-1.9.2\n"
          ]
        }
      ],
      "source": [
        "'''Word Cloud of Job Titles in LinkedIn Tech Jobs Dataset'''\n",
        "\n",
        "'''Step 1'''\n",
        "\n",
        "!pip install wordcloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "id": "IFmgRW3e6Fqp",
        "outputId": "64555314-c538-46d9-f5ed-86bb5d4783f7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-14-e97b73f60e3e>:17: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
            "  plt.show()\n"
          ]
        }
      ],
      "source": [
        "'''step 2'''\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "# Assuming 'Job_Title' is the column with job names\n",
        "\n",
        "'''step 3'''\n",
        "#extracts the job titles\n",
        "job_df = df['Designation']\n",
        "all_job_titles = ' '.join(job_df)\n",
        "\n",
        "'''step 4'''\n",
        "\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_job_titles)\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SR0dmUFz_XBK"
      },
      "source": [
        "# Addditional details and explanations :\n",
        "**step 1 : Install wordcloud**\n",
        "> The command !pip install wordcloud is used to install the WordCloud package from the Python Package Index (PyPI). WordCloud is a Python package that allows us to generate word clouds, which are visual representations of text data. Word clouds are often used to display the frequency of words in a text dataset.\n",
        "\n",
        "**step 2 : Importing workcloud**\n",
        ">This step imports the WordCloud library. The WordCloud library is used to generate word clouds, which are visual representations of text data. Word clouds are often used to display the frequency of words in a text dataset.\n",
        "\n",
        "**step 3 : Extracts the job titles**\n",
        ">This step extracts the job titles from the LinkedIn Tech Jobs Dataset. The job titles are extracted from the 'Designation' column of the DataFrame and stored in a new variable called 'job_df'. Subsequently, a string containing all of the job titles in the LinkedIn Tech Jobs Dataset is created by joining the job titles together with spaces and stored in a new variable called 'all_job_titles'.\n",
        "\n",
        "**step 4 : Visualization**\n",
        ">This step creates a WordCloud object using the imported WordCloud library and passes it the parameters for width, height, and background color. The generate() method is then called on the WordCloud object, passing it the 'all_job_titles' string, which generates a word cloud from the extracted job titles. Next, a figure is created using the figure() function from the matplotlib library with the specified dimensions of 10 inches in width and 5 inches in height. The generated word cloud is then displayed using the imshow() function from the matplotlib library, setting the interpolation parameter to 'bilinear' to improve image quality. Finally, the axis labels are turned off using the axis('off') function from the matplotlib library, and the figure is displayed using the show() function from the matplotlib library.\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
